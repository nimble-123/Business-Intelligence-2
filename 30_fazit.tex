\section{Fazit}
\paragraph{Zusammenfassung}$\;$ \\
Hadoop ist ein Framework, das konzipiert wurde um große Datenmengen schnell und effizient verarbeiten zu können. Das Framework besteht aus vielen Komponenten, die bestimmte Funktionen übernehmen. So können intensive Rechenprozesse mit großen Datenmengen von großen Computerclustern bearbeitet werden. Ein besonderer Bestandteil von Hadoop ist der $MapReduce$-Algorithmus und das Hadoop Distributed File System. Diese beiden Basis-Komponenten unterstützen Hadoop bei seinen Aufgaben.
\paragraph{Ausblick}$\;$ \\
Das Thema ist aktuell. Große Firmen wie Facebook, AOL oder IBM verwenden Hadoop. Da es plattformunabhängig ist, kann es in vielen Szenarien verwendet werden. Die Stärken spielt Hadoop aus, wenn große Datenmengen verarbeitet werden müssen. Hadoop wird besonders interessant im Kontext von Big Data. Unternehmen die große Datenmengen analysieren wollen, können Hadoop verwenden. Ein Unternehmen mit relativ wenig Datenmengen, wird nicht durch einen Einsatz von Hadoop profitieren.