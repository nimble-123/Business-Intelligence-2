\section{Einleitung}
Apache Hadoop ist ein Framework für skalierbare, verteilt arbeitende Software. Es ermöglicht u.A. effiziente Big-Data-Analysen. Das Framework ist allerdings eine relativ neue Technologie (vgl.:\cite{avn}). Im Rahmen des Moduls \glqq Business Intelligence II. \grqq wird dieses Thema bearbeitet. Das Ziel dieser Ausarbeitung ist das Erarbeiten des aktuellen Stand der Technik. Außerdem soll bestimmt werden, aus welchen Komponenten das Hadoop Framework besteht. Die Ausarbeitung ist so strukturiert, das als Erstes, das Hadoop Framework bearbeitet wird. Dabei wird analysiert aus welchen Komponenten Hadoop besteht. Hadoop verwendet ein bestimmtes File System zum Arbeiten. Das Hadoop Distributed File System wird im zweiten Unterkapitel beschrieben. Im dritten Unterkapitel wird das MapReduce-Verfahren behandelt. Dabei soll geklärt werden, wie das Verfahren funktioniert. Im abschließenden Fazit werden die Ergebnisse der Ausarbeitung betrachtet.