\section{Einleitung}
Apache Hadoop ist ein Framework für skalierbare, verteilt arbeitende Software. Es ermöglicht unter anderem effiziente Big-Data-Analysen. Das Framework ist allerdings eine relativ neue Technologie (vgl. \cite{avn}). Im Rahmen des Moduls \glqq Business Intelligence II\grqq{} wird dieses Thema bearbeitet. Das Ziel dieser Ausarbeitung ist das Erarbeiten des aktuellen Standes der Technik. Außerdem soll bestimmt werden, aus welchen Komponenten das Hadoop Framework besteht. Die Ausarbeitung ist so strukturiert, das als Erstes, das Hadoop Framework bearbeitet wird. Dabei wird analysiert aus welchen Komponenten Hadoop besteht. Hadoop verwendet ein bestimmtes Dateiystem zum Arbeiten. Das Hadoop Distributed File System wird im zweiten Unterkapitel beschrieben. Im dritten Unterkapitel wird das MapReduce-Verfahren behandelt. Dabei soll geklärt werden, wie das Verfahren funktioniert. Im abschließenden Fazit werden die Ergebnisse der Ausarbeitung betrachtet.